{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Importing Libraries:\n",
    "=="
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils import data\n",
    "from torchvision.transforms import functional as F\n",
    "import torchvision\n",
    "\n",
    "from sklearn.metrics import jaccard_score as jsc\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib.path import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('GPU is available!')\n",
    "    device = \"cuda\"\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor') \n",
    "else:\n",
    "    print('GPU is not available!')\n",
    "    device = \"cpu\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparams:\n",
    "=="
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# depth of UNet = 3\n",
    "learning_rate = 1e-3\n",
    "epoch_number = 5\n",
    "batch_size = 1 # make it 2-4 once bug fixing is over\n",
    "train_size = 100 # divide train:val by 70:30 probably\n",
    "test_size = 25 # if we break the super test image into individual images to run our model on\n",
    "img_size = 768 # dimensions to resize input 750x750 into (power of 2^(depth+\\delta) necessary for model to work)\n",
    "max_channels = 512 # gives error if divided by 2 {or more}, not technically max_channel... number of channels after first conv is max_channels / 2^4\n",
    "gradient_clip = 50 # tried 10 and 100 too   \n",
    "dice_alpha = 1e-5 # maybe try 1e-4 and 1.0\n",
    "pretrained = True\n",
    "freeze_layers = 0 # for pretrained model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading Data:\n",
    "=="
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def transform(x):\n",
    "    return  torchvision.transforms.functional.to_tensor(x)\n",
    "\n",
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def float_maker(self, a):\n",
    "        if a[0]=='(':\n",
    "            return a[1:]\n",
    "        if a[-1]==')':\n",
    "            return a[:-1]\n",
    "        return a\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.image_names= os.listdir('./Data_ML/image_chips/')\n",
    "        # self.image_names= os.listdir('/content/drive/MyDrive/Data_ML/image_chips/') for google.colab\n",
    "    def __getitem__(self, index):\n",
    "        image = plt.imread(os.path.join(os.getcwd(),'Data_ML','image_chips',self.image_names[index]))\n",
    "        mask = pd.read_csv(os.path.join(os.getcwd(),'Data_ML','target_feature_AOI',self.image_names[index][:-4]+'.csv'))\n",
    "        # for google.colab\n",
    "        # image = plt.imread(os.path.join('/content/drive/MyDrive','Data_ML','image_chips',self.image_names[index]))\n",
    "        # mask = pd.read_csv(os.path.join('/content/drive/MyDrive','Data_ML','target_feature_AOI',self.image_names[index][:-4]+'.csv'))\n",
    "        coordinates = mask.WKT.values[0][16:-3].split(',')\n",
    "        points = ([tuple(self.float_maker(i)  for i in x.split()) for x in coordinates])\n",
    "        h , w = 750, 750\n",
    "        point_path = Path(points)\n",
    "        x, y = np.mgrid[:h, :w]\n",
    "        y = - y\n",
    "        coors=np.hstack((x.reshape(-1, 1), y.reshape(-1,1)))\n",
    "        masked_image = (point_path.contains_points(coors))\n",
    "        first = torchvision.transforms.Resize((img_size,img_size))(transform(image))\n",
    "        second = (torchvision.transforms.functional.to_tensor(masked_image.reshape(h,w)).int())\n",
    "        return (first,second)\n",
    "    def __len__(self):\n",
    "        return train_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# input training images are 750x750x3 and are 100 in number\n",
    "# test image is 5x5 images (hence 3750x3750x3), we need to output a masked image with 0s and 1s\n",
    "input_channels = 3 \n",
    "output_classes = 1\n",
    "\n",
    "train_dataLoader = data.DataLoader(\n",
    "    DataSet(), #need to get DataSet to make train and CV both\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers= 0\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating Module:\n",
    "=="
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class UNetConv(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv2(x)\n",
    "class DownConv(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        self.conv_down = nn.Sequential(\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            UNetConv(input_channels, output_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv_down(x)\n",
    "class UpConv(nn.Module): \n",
    "    def __init__(self, input_channels, output_channels, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv_up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(input_channels , input_channels // 2, kernel_size=2, stride=2, padding = padding),\n",
    "        )\n",
    "        self.conv_level = nn.Sequential(\n",
    "            UNetConv(input_channels,output_channels) \n",
    "        )\n",
    "    def forward(self, x1, x2):\n",
    "        return self.conv_level(torch.cat([x2,self.conv_up(x1)],dim = 1)) \n",
    "class LastConv(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        self.conv_final = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv_final(x)\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels, max_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.current_channels = max_channels // 2**4\n",
    "        self.start = UNetConv(input_channels, self.current_channels)\n",
    "        self.current_channels = self.current_channels*2\n",
    "        self.down1 = DownConv(self.current_channels // 2 , self.current_channels)\n",
    "        self.current_channels = self.current_channels*2\n",
    "        self.down2 = DownConv(self.current_channels // 2 , self.current_channels)\n",
    "        self.current_channels = self.current_channels*2\n",
    "        self.down3 = DownConv(self.current_channels // 2 , self.current_channels)\n",
    "        #self.current_channels = self.current_channels*2\n",
    "        #self.down4 = DownConv(self.current_channels // 2 , self.current_channels)\n",
    "        #self.current_channels = self.current_channels // 2\n",
    "        #self.up1 = UpConv(self.current_channels * 2, self.current_channels) \n",
    "        self.current_channels = self.current_channels // 2\n",
    "        self.up2 = UpConv(self.current_channels * 2, self.current_channels)\n",
    "        self.current_channels = self.current_channels // 2\n",
    "        self.up3 = UpConv(self.current_channels * 2, self.current_channels)\n",
    "        self.current_channels = self.current_channels // 2\n",
    "        self.up4 = UpConv(self.current_channels * 2, self.current_channels)\n",
    "        self.final = LastConv(self.current_channels,output_classes)\n",
    "    def forward(self, x):\n",
    "        x1 = self.start(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        #x5 = self.down4(x4)\n",
    "        #y1 = self.up1(x5,x4)\n",
    "        y2 = self.up2(x4,x3) #x4 was y1 earlier\n",
    "        y3 = self.up3(y2,x2) \n",
    "        x = self.up4(y3,x1)\n",
    "        return torch.sigmoid(self.final(x))\n",
    "\n",
    "if(pretrained):\n",
    "    Model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
    "    Model = Model.train(True).to(device)    \n",
    "    for parents in Model.children():\n",
    "        for count, child in enumerate(parents.children()):\n",
    "            if count == freeze_layers:\n",
    "                break\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad=False\n",
    "else:\n",
    "    Model = UNet(input_channels, max_channels)\n",
    "    Model = Model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training Model:\n",
    "=="
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "epochs = epoch_number\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = dice_alpha #maybe try 1e-4\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.size() == y_true.size()\n",
    "        y_pred = y_pred[:, 0].contiguous().view(-1)\n",
    "        y_true = y_true[:, 0].contiguous().view(-1)\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        dsc = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n",
    "        return 1. - dsc\n",
    "\n",
    "loss_func = DiceLoss()\n",
    "optimizer = optim.Adam(Model.parameters(),lr=learning_rate)\n",
    "acc_sanity = jsc\n",
    "trainLoss = []\n",
    "sanityAccc = []\n",
    "for epoch in range(epochs):  \n",
    "    epochStart = time.time()\n",
    "    runningLoss = 0.0\n",
    "    sanityAcc = 0.0\n",
    "    i = 0\n",
    "    for inputs, labels in tqdm(train_dataLoader): \n",
    "        if(i==3):\n",
    "            break\n",
    "        i+=1\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if(pretrained):\n",
    "            inputs = torchvision.transforms.Resize((256,256))(inputs)\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = Model(inputs) \n",
    "        outputs = torchvision.transforms.Resize((750,750))(outputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward() \n",
    "        acc_san = acc_sanity(torch.round(outputs).cpu().detach().numpy().reshape(-1), labels.cpu().detach().numpy().reshape(-1))\n",
    "        nn.utils.clip_grad_norm_(Model.parameters(), gradient_clip)\n",
    "        optimizer.step()\n",
    "        runningLoss += loss.item()\n",
    "        sanityAcc += acc_san.item()\n",
    "    runningLoss *= batch_size/train_size\n",
    "    sanityAcc *= batch_size/train_size\n",
    "    sanityAccc.append(sanityAcc)\n",
    "    trainLoss.append(runningLoss)\n",
    "    epochEnd = time.time()-epochStart\n",
    "    print('Iteration: {:.0f} /{:.0f}  ;  Training Loss: {:.6f} ; Jaccard Score: {:.6f} ; Time consumed: {:.0f}m {:.0f}s '\\\n",
    "        .format(epoch + 1, epochs, runningLoss, sanityAcc, epochEnd//60, epochEnd%60))   \n",
    "print('Finished Training')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing Model:\n",
    "=="
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_loss = 0.0\n",
    "for inputs, labels in tqdm(test_dataLoader):\n",
    "    outputs = Model(inputs)\n",
    "    loss = loss_func(outputs, labels)\n",
    "    test_loss += loss.item()\n",
    "test_loss /= len(inputs)\n",
    "print(test_loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d32307ab60c186a8047d52516f214c86ffc4fca0a07f2d9b7e263da528082b42"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('shims': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}