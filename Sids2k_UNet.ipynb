{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Importing Libraries:\n",
    "=="
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils import data\n",
    "from torchvision.transforms import functional as F\n",
    "import torchvision\n",
    "import shutil\n",
    "\n",
    "from sklearn.metrics import jaccard_score as jsc\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib.path import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('GPU is available!')\n",
    "    device = \"cuda\"\n",
    "    #torch.set_default_tensor_type('torch.cuda.FloatTensor') (else HPC doesn't work)\n",
    "else:\n",
    "    print('GPU is not available!')\n",
    "    device = \"cpu\""
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU is not available!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparams:\n",
    "=="
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# depth of UNet = 4\n",
    "learning_rate = 1e-4\n",
    "epoch_number = 50\n",
    "batch_size = 32\n",
    "train_size = 100 # divide train:val by 70:30 probably\n",
    "aug_train_size = 500\n",
    "test_size = 25 \n",
    "img_size = 256 # dimensions to resize input 750x750 into (power of 2^(depth+\\delta) necessary for model to work)\n",
    "min_channels = 16 \n",
    "gradient_clip = 1e9 # basically unactive\n",
    "dropout_percent = 35 \n",
    "dice_alpha = 1e-3 # maybe try 1e-4 and 1.0\n",
    "pretrained = True\n",
    "pretrained_model = 'unet'\n",
    "freeze_layers = 2 # for pretrained model\n",
    "name = 'i_test'\n",
    "PATH = os.path.join(os.getcwd(),'models/',name+'.pt')\n",
    "bestPATH = os.path.join(os.getcwd(),'models/best_model_'+name+'.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading Data:\n",
    "=="
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def transform(x):\n",
    "    return  torchvision.transforms.functional.to_tensor(x)\n",
    "\n",
    "def augmenter(image, segmap, index):\n",
    "    mode = index/(aug_train_size/train_size)\n",
    "    if(mode == 1):\n",
    "        image, segmap = iaa.Sequential([\n",
    "            iaa.CoarseDropout(0.1, size_percent=0.2),\n",
    "            iaa.Affine(rotate=(-30, 30)),\n",
    "            iaa.ElasticTransformation(alpha=20, sigma=2)\n",
    "        ])(image=image, segmentation_maps=segmap)\n",
    "    elif(mode == 2):\n",
    "        image, segmap = iaa.Sequential([\n",
    "            iaa.Dropout([0.05, 0.2]),      \n",
    "            iaa.Sharpen((0.0, 1.0)),       \n",
    "            iaa.Affine(rotate=(-90, 90)),  \n",
    "            iaa.ElasticTransformation(alpha=50, sigma=5)  \n",
    "        ], random_order=True)(image=image, segmentation_maps=segmap)\n",
    "    elif(mode == 3):\n",
    "        image, segmap = iaa.Sequential([\n",
    "            iaa.Fliplr(p=0.8),\n",
    "            iaa.Flipud(p=0.8),\n",
    "            iaa.Affine(rotate = (-180,180))\n",
    "        ])(image=image, segmentation_maps=segmap)\n",
    "    elif(mode == 4):\n",
    "        image, segmap = iaa.Sequential([\n",
    "            iaa.Fliplr(p=1),\n",
    "            iaa.Flipud(p=0.4),\n",
    "            iaa.Affine(scale=(0.5,1.5),rotate=(-120,120)),\n",
    "            iaa.WithHueAndSaturation([\n",
    "                iaa.WithChannels(0, iaa.Add((-30, 10))),\n",
    "                iaa.WithChannels(1, [\n",
    "                    iaa.Multiply((0.5, 1.5)),\n",
    "                    iaa.LinearContrast((0.75, 1.25))\n",
    "                ])\n",
    "            ])\n",
    "        ])(image=image, segmentation_maps=segmap)\n",
    "    return image, segmap\n",
    "    \n",
    "\n",
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def float_maker(self, a):\n",
    "        if a[0]=='(':\n",
    "            return a[1:]\n",
    "        if a[-1]==')':\n",
    "            return a[:-1]\n",
    "        return a\n",
    "    def __init__(self, mode):\n",
    "        super().__init__()\n",
    "        self.image_names= os.listdir('./Data_ML/image_chips/')\n",
    "        self.mode = mode\n",
    "        # self.image_names= os.listdir('/content/drive/MyDrive/Data_ML/image_chips/') for google.colab\n",
    "    def __getitem__(self, index):\n",
    "        if(self.mode == 'Train'):\n",
    "            image = plt.imread(os.path.join(os.getcwd(),'Data_ML','image_chips',self.image_names[index%train_size]))\n",
    "            mask = pd.read_csv(os.path.join(os.getcwd(),'Data_ML','target_feature_AOI',self.image_names[index%train_size][:-4]+'.csv'))\n",
    "            # for google.colab\n",
    "            # image = plt.imread(os.path.join('/content/drive/MyDrive','Data_ML','image_chips',self.image_names[index%train_size]))\n",
    "            # mask = pd.read_csv(os.path.join('/content/drive/MyDrive','Data_ML','target_feature_AOI',self.image_names[index%train_size][:-4]+'.csv'))\n",
    "            coordinates = mask.WKT.values[0][16:-3].split(',')\n",
    "            points = ([tuple(self.float_maker(i)  for i in x.split()) for x in coordinates])\n",
    "            h , w = 750, 750\n",
    "            point_path = Path(points)\n",
    "            x, y = np.mgrid[:h, :w]\n",
    "            y = - y\n",
    "            coors=np.hstack((x.reshape(-1, 1), y.reshape(-1,1)))\n",
    "            masked_image = (point_path.contains_points(coors)).reshape(h,w)\n",
    "            segmap = SegmentationMapsOnImage(masked_image, shape = masked_image.shape)\n",
    "            image, segmap_aug = augmenter(image, segmap, index)\n",
    "            masked_image = segmap_aug.get_arr()\n",
    "            first = torchvision.transforms.Resize((img_size,img_size))(transform(image.copy()))\n",
    "            second = (torchvision.transforms.functional.to_tensor(masked_image).int())\n",
    "            return (first,second)\n",
    "        else:\n",
    "            image = plt.imread(os.path.join(os.getcwd(),'mosaic_test.jpg'))\n",
    "            x,y = (index)//5  , (index+1)%5\n",
    "            if(y==0):   \n",
    "                y=5\n",
    "            img_crop=image[(x)*750:(x+1)*750,(y-1)*750:(y)*750,:]\n",
    "            return torchvision.transforms.Resize((img_size,img_size))(transform(img_crop))\n",
    "    def __len__(self):\n",
    "        if(self.mode=='Train'):\n",
    "            return aug_train_size\n",
    "        else:\n",
    "            return test_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# input training images are 750x750x3 and are 100 in number\n",
    "# test image is 5x5 images (hence 3750x3750x3), we need to output a masked image with 0s and 1s\n",
    "input_channels = 3 \n",
    "output_classes = 1\n",
    "\n",
    "train_dataLoader = data.DataLoader(\n",
    "    DataSet('Train'), #need to get DataSet to make train and CV both\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers= 0\n",
    ")\n",
    "test_dataLoader = data.DataLoader(\n",
    "    DataSet('Test'),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating Module:\n",
    "=="
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class UNetConv(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv2(x)\n",
    "class DownConv(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        self.conv_down = nn.Sequential(\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            UNetConv(input_channels, output_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv_down(x)\n",
    "class UpConv(nn.Module): \n",
    "    def __init__(self, input_channels, output_channels, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv_up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(input_channels , input_channels // 2, kernel_size=2, stride=2, padding = padding),\n",
    "        )\n",
    "        self.conv_level = nn.Sequential(\n",
    "            UNetConv(input_channels,output_channels) \n",
    "        )\n",
    "    def forward(self, x1, x2):\n",
    "        return self.conv_level(torch.cat([x2,self.conv_up(x1)],dim = 1)) \n",
    "class LastConv(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        self.conv_final = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv_final(x)\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels, min_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.current_channels = min_channels\n",
    "        self.start = UNetConv(input_channels, self.current_channels)\n",
    "        self.current_channels = self.current_channels*2\n",
    "        self.down1 = DownConv(self.current_channels // 2 , self.current_channels)\n",
    "        self.current_channels = self.current_channels*2\n",
    "        self.down2 = DownConv(self.current_channels // 2 , self.current_channels)\n",
    "        self.current_channels = self.current_channels*2\n",
    "        self.down3 = DownConv(self.current_channels // 2 , self.current_channels)\n",
    "        self.current_channels = self.current_channels*2\n",
    "        self.down4 = DownConv(self.current_channels // 2 , self.current_channels)\n",
    "        self.current_channels = self.current_channels // 2\n",
    "        self.up1 = UpConv(self.current_channels * 2, self.current_channels) \n",
    "        self.current_channels = self.current_channels // 2\n",
    "        self.up2 = UpConv(self.current_channels * 2, self.current_channels)\n",
    "        self.current_channels = self.current_channels // 2\n",
    "        self.up3 = UpConv(self.current_channels * 2, self.current_channels)\n",
    "        self.current_channels = self.current_channels // 2\n",
    "        self.up4 = UpConv(self.current_channels * 2, self.current_channels)\n",
    "        self.final = LastConv(self.current_channels,output_classes)\n",
    "    def forward(self, x):\n",
    "        x1 = self.start(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        y1 = self.up1(x5,x4)\n",
    "        y2 = self.up2(x4,x3) \n",
    "        y3 = self.up3(y2,x2) \n",
    "        x = self.up4(y3,x1)\n",
    "        x = torch.nn.Dropout2d(p=dropout_percent/100)(x)\n",
    "        return torch.sigmoid(self.final(x))\n",
    "\n",
    "if(pretrained):\n",
    "    Model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
    "    Model = Model.train(True).to(device)    \n",
    "    for parents in Model.children():\n",
    "        for count, child in enumerate(parents.children()):\n",
    "            if count == freeze_layers:\n",
    "                break\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad=False\n",
    "else:\n",
    "    Model = UNet(input_channels, min_channels)\n",
    "    Model = Model.to(device)\n",
    "\n",
    "if(use_gpu):\n",
    "   Model = nn.DataParallel(Model, device_ids = [i for i in range(torch.cuda.device_count())]).to(0)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(Model.parameters(),lr=learning_rate)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /Users/siddhant/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    f_path = checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)\n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    if(use_gpu==False):\n",
    "        checkpoint = torch.load(checkpoint_fpath,map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    loss = checkpoint['loss']\n",
    "    acc = checkpoint['accuracy']\n",
    "    return model, optimizer, checkpoint['epoch'], loss, acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training Model:\n",
    "=="
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# maybe make into a functional model 'def train' so that i can use checkpoints as well\n",
    "runningLoss_min = 1e9 # take input for this from previous training if using functional model and training multiple times\n",
    "epochs = epoch_number\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = dice_alpha #maybe try 1e-4\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.size() == y_true.size()\n",
    "        y_pred = y_pred[:, 0].contiguous().view(-1)\n",
    "        y_true = y_true[:, 0].contiguous().view(-1)\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        dsc = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n",
    "        return 1. - dsc\n",
    "\n",
    "loss_func = DiceLoss()\n",
    "acc_sanity = jsc\n",
    "trainLoss = []\n",
    "sanityAccc = []\n",
    "liveloss = PlotLosses()\n",
    "for epoch in range(epochs):  \n",
    "    epochStart = time.time()\n",
    "    runningLoss = 0.0\n",
    "    sanityAcc = 0.0\n",
    "    i = 0\n",
    "    for inputs, labels in tqdm(train_dataLoader): \n",
    "        if(i==3):\n",
    "            break\n",
    "        inputs = inputs.to(device)\n",
    "        if(use_gpu):\n",
    "            inputs = inputs.to(0)\n",
    "        else:\n",
    "            i=i+1\n",
    "        labels = labels.to(device)\n",
    "        if(pretrained):\n",
    "            inputs = torchvision.transforms.Resize((256,256))(inputs)\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = Model(inputs) \n",
    "        outputs = torchvision.transforms.Resize((750,750))(outputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward() \n",
    "        acc_san = acc_sanity(torch.round(outputs).cpu().detach().numpy().reshape(-1), labels.cpu().detach().numpy().reshape(-1))\n",
    "        nn.utils.clip_grad_norm_(Model.parameters(), gradient_clip)\n",
    "        optimizer.step()\n",
    "        runningLoss += loss.item()\n",
    "        sanityAcc += acc_san.item()\n",
    "    runningLoss *= batch_size/aug_train_size\n",
    "    sanityAcc *= batch_size/aug_train_size\n",
    "    logs = {}\n",
    "    logs['loss'] = runningLoss\n",
    "    logs['accuracy'] = sanityAcc\n",
    "    liveloss.update(logs)\n",
    "    liveloss.send()\n",
    "    checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': runningLoss,\n",
    "            'state_dict': Model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'accuracy' : sanityAcc,\n",
    "        }\n",
    "    save_ckp(checkpoint, False, PATH, bestPATH)\n",
    "    # do this for validation loss \n",
    "    if runningLoss <= runningLoss_min:\n",
    "        print('Loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(runningLoss_min,runningLoss))\n",
    "        save_ckp(checkpoint, True, PATH, bestPATH)\n",
    "        runningLoss_min = runningLoss\n",
    "    sanityAccc.append(sanityAcc)\n",
    "    trainLoss.append(runningLoss)\n",
    "    epochEnd = time.time()-epochStart\n",
    "    print('Iteration: {:.0f} /{:.0f}  ;  Training Loss: {:.6f} ; Jaccard Score: {:.6f} ; Time consumed: {:.0f}m {:.0f}s '\\\n",
    "        .format(epoch + 1, epochs, runningLoss, sanityAcc, epochEnd//60, epochEnd%60))   \n",
    "print('Finished Training')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0eec87fe1a4541e5927b9670dd8195ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/siddhant/.pyenv/versions/3.8.2/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot([i+1 for i in range(epochs)],trainLoss)\n",
    "plt.plot([i+1 for i in range(epochs)], sanityAccc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing Model:\n",
    "=="
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Model, optimizer, epoch, loss, acc = load_ckp(bestPATH,Model,optimizer) #use bestPATH with name test, and don't overwrite it via above script\n",
    "Model.eval()\n",
    "i = 0\n",
    "for inputs in tqdm(test_dataLoader):\n",
    "    i+=1\n",
    "    inputs = inputs.to(device)\n",
    "    if(use_gpu):\n",
    "        inputs = inputs.to(0)\n",
    "    else:\n",
    "        if(i==5):\n",
    "            break\n",
    "    outputs = Model(inputs)\n",
    "    outputs = torchvision.transforms.Resize((750,750))(outputs).cpu().detach().numpy().reshape((750,750))\n",
    "    print(outputs)\n",
    "    plt.imshow(outputs)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(5,5)\n",
    "    print(\"Index \"+str(i)+\": \")\n",
    "    plt.show()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'load_ckp' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-48db6da742af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_ckp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbestPATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#use bestPATH with name test, and don't overwrite it via above script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_ckp' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#make a new dataloader with batchsize 1 for training data\n",
    "Model, optimizer, epoch, loss, acc = load_ckp(bestPATH,Model,optimizer) #use bestPATH with name test, and don't overwrite it via above script\n",
    "Model.eval()\n",
    "i=0\n",
    "for inputs,labels in tqdm(train_dataLoader):\n",
    "    if(i==10):\n",
    "        break\n",
    "    i+=1\n",
    "    inputs = inputs.to(device)\n",
    "    if(use_gpu):\n",
    "        inputs = inputs.to(0)\n",
    "    labels = labels.to(device)\n",
    "    if(pretrained):\n",
    "        inputs = torchvision.transforms.Resize((256,256))(inputs)\n",
    "    outputs = Model(inputs)\n",
    "    outputs = torchvision.transforms.Resize((750,750))(outputs).cpu().detach().numpy().reshape((750,750))\n",
    "    plt.imshow(outputs)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(5,5)\n",
    "    print(\"Index \"+str(i)+\": \")\n",
    "    plt.show()\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d32307ab60c186a8047d52516f214c86ffc4fca0a07f2d9b7e263da528082b42"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}